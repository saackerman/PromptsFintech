# Quick Start Guide

Get started with comparing AI systems using fintech prompts in just a few minutes.

## 5-Minute Quick Start

### Step 1: Choose Your Test Category

Pick one category based on what you want to evaluate:
- **Fraud Detection** - Pattern recognition and security
- **Risk Assessment** - Analytical and decision-making skills
- **Financial Analysis** - Calculation and interpretation abilities
- **Compliance** - Regulatory knowledge
- **Customer Service** - Communication and problem-solving
- **Investment Advice** - Financial planning expertise
- **Payment Processing** - Technical infrastructure knowledge
- **Credit Scoring** - Credit evaluation skills

### Step 2: Select a Prompt

Start with a **Basic** or **Intermediate** difficulty prompt from your chosen category.

Example: Try "Credit Card Fraud Detection" from the Fraud Detection category (Basic level).

### Step 3: Test Multiple AI Systems

1. Copy the prompt text exactly as written
2. Paste it into AI System #1 (e.g., ChatGPT)
3. Save the response
4. Repeat with AI System #2 (e.g., Claude)
5. Repeat with AI System #3 (e.g., Gemini)

### Step 4: Quick Evaluation

Use these three simple questions:

1. **Is it accurate?** - Check against the "Expected Evaluation Points"
2. **Is it complete?** - Does it answer all parts of the prompt?
3. **Is it practical?** - Could you actually use this advice?

### Step 5: Record Results

Create a simple comparison:

```
Prompt: [Name]
Winner: [AI that performed best]
Why: [Brief reason]
```

## 30-Minute Deep Dive

### Extended Testing Protocol

1. **Choose 3 prompts** from different categories
2. **Test with 2-3 AI systems**
3. **Use the evaluation framework** from the Evaluation Guide
4. **Score each response** on the 5 criteria (0-10 each)
5. **Calculate totals** and compare

### Sample Test Suite

**Beginner Test Suite** (30 minutes):
1. Credit Card Fraud Detection (Fraud Detection - Basic)
2. Cash Flow Analysis (Financial Analysis - Basic)
3. Disputed Transaction (Customer Service - Basic)

**Intermediate Test Suite** (45 minutes):
1. Suspicious Transaction Pattern (Fraud Detection - Intermediate)
2. Credit Risk Evaluation (Risk Assessment - Intermediate)
3. KYC/AML Compliance (Compliance - Intermediate)

**Advanced Test Suite** (60 minutes):
1. Account Takeover Scenario (Fraud Detection - Advanced)
2. Market Risk Analysis (Risk Assessment - Advanced)
3. Data Privacy Compliance (Compliance - Advanced)

## Common Use Cases

### Use Case 1: Evaluating AI for Customer Support

**Recommended Prompts:**
- All Customer Service prompts
- Basic Fraud Detection prompts
- Credit Score Improvement Advice

**Focus Areas:**
- Communication quality
- Empathy and tone
- Problem-solving
- Policy knowledge

### Use Case 2: Testing AI for Financial Analysis

**Recommended Prompts:**
- All Financial Analysis prompts
- Risk Assessment prompts
- Valuation Analysis

**Focus Areas:**
- Calculation accuracy
- Ratio interpretation
- Business context understanding
- Practical recommendations

### Use Case 3: Compliance and Risk Roles

**Recommended Prompts:**
- All Compliance prompts
- Fraud Detection prompts
- Risk Assessment prompts

**Focus Areas:**
- Regulatory knowledge
- Risk identification
- Documentation requirements
- Ethical considerations

### Use Case 4: Investment Advisory

**Recommended Prompts:**
- All Investment Advice prompts
- Financial Analysis prompts
- Risk Assessment prompts

**Focus Areas:**
- Tax efficiency
- Risk-adjusted thinking
- Time horizon considerations
- Fiduciary awareness

## Tips for Effective Testing

### Do's ‚úÖ
- Use prompts exactly as written for consistency
- Test at least 2-3 AI systems for comparison
- Take notes on unexpected or impressive responses
- Try prompts multiple times to check consistency
- Update your findings as AI systems improve

### Don'ts ‚ùå
- Don't modify prompts when comparing (loses consistency)
- Don't rely on a single prompt to judge an entire category
- Don't ignore obvious errors or red flags
- Don't treat AI responses as professional advice
- Don't share sensitive real data with AI systems

## Interpreting Results

### Score Ranges

**45-50 points (Excellent)**
- AI demonstrates strong expertise
- Suitable for professional reference (with verification)
- High reliability for this category

**35-44 points (Good)**
- AI shows solid understanding
- Useful for general guidance
- Some gaps or minor errors present

**25-34 points (Adequate)**
- Basic competence demonstrated
- Requires significant verification
- Better for brainstorming than final answers

**Below 25 points (Poor)**
- Significant gaps in knowledge
- Not recommended for this use case
- Consider different AI or category

### What to Look For

**Green Flags** üü¢
- Acknowledges limitations and uncertainties
- Provides specific, actionable recommendations
- Considers multiple perspectives
- Cites or references regulatory requirements
- Warns about risks and compliance issues

**Red Flags** üî¥
- Makes definitive claims without qualification
- Provides advice that could be illegal or harmful
- Ignores obvious risks
- Shows bias in credit or hiring decisions
- Makes up regulations or statistics

## Next Steps

### After Your First Test
1. Try prompts from 2-3 different categories
2. Document patterns in AI strengths/weaknesses
3. Test more advanced prompts as you get comfortable
4. Consider contributing new prompts to the repository

### Share Your Findings
- Document which AIs work best for which categories
- Note any particularly impressive or concerning responses
- Share edge cases or interesting variations
- Contribute to improving the prompt collection

## Troubleshooting

**Problem:** AI gives very different answers each time
- **Solution:** This is normal. Test 3-5 times and look for patterns

**Problem:** All AIs score low on a prompt
- **Solution:** Prompt might be too complex or ambiguous. Try a different one.

**Problem:** Can't decide which response is better
- **Solution:** Use the detailed evaluation framework with numerical scoring

**Problem:** AI refuses to answer or gives disclaimer
- **Solution:** This might be appropriate! Some prompts should trigger caution.

**Problem:** Scores don't match subjective impression
- **Solution:** Review the rubric. Sometimes technically correct answers feel less helpful.

## Resources in This Repository

- **README.md** - Overview and structure
- **templates/TEMPLATE.md** - Create your own prompts
- **evaluation/GUIDE.md** - Detailed evaluation methodology
- **prompts/[category]/README.md** - Category-specific prompts

## Quick Reference Card

```
1. Pick a category
2. Choose a prompt (start with Basic)
3. Copy prompt to 2-3 AIs
4. Compare responses
5. Score: Accuracy, Completeness, Practicality
6. Record winner and why
7. Try another category
```

Happy testing! üöÄ
